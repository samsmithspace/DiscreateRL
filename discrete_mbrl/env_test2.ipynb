{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-04T20:16:09.945644Z",
     "start_time": "2025-07-04T20:16:09.940816Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from env_helpers import make_env\n",
    "from visualization import states_to_imgs\n",
    "# Add the parent directory to path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "env = make_env('MiniGrid-Empty-6x6-v0')\n",
    "\n",
    "# Get initial observation\n",
    "obs = env.reset()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T22:38:45.597855Z",
     "start_time": "2025-07-09T22:38:45.278741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the parent directory to path\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from env_helpers import make_env\n",
    "from visualization import states_to_imgs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = make_env('MiniGrid-Empty-6x6-v0')\n",
    "\n",
    "# Get initial observation\n",
    "obs = env.reset()\n",
    "\n",
    "\n",
    "\n",
    "obs = obs[0]\n",
    "\n",
    "print(f\"Raw observation type: {type(obs)}\")\n",
    "print(f\"Raw observation shape: {obs.shape}\")\n",
    "print(f\"Raw observation dtype: {obs.dtype}\")\n",
    "print(f\"Raw observation range: [{obs.min():.3f}, {obs.max():.3f}]\")\n",
    "\n",
    "# Check if observation has proper spatial dimensions\n",
    "if len(obs.shape) == 3:\n",
    "    print(f\"Channels: {obs.shape[0]}, Height: {obs.shape[1]}, Width: {obs.shape[2]}\")\n",
    "\n",
    "# Visualize each channel\n",
    "fig, axes = plt.subplots(1, obs.shape[0], figsize=(12, 4))\n",
    "if obs.shape[0] == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(obs.shape[0]):\n",
    "    channel = obs[i]\n",
    "    axes[i].imshow(channel, cmap='viridis')\n",
    "    axes[i].set_title(f'Channel {i}')\n",
    "    axes[i].axis('off')\n",
    "    print(f\"Channel {i} unique values: {np.unique(channel)}\")\n",
    "\n",
    "plt.suptitle('Raw Observation Channels')\n",
    "plt.tight_layout()\n",
    "plt.savefig('debug_raw_channels.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Visualize full RGB if 3 channels\n",
    "if obs.shape[0] == 3:\n",
    "    rgb_img = obs.transpose(1, 2, 0)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(rgb_img.clip(0, 1))\n",
    "    plt.title('Raw RGB Observation')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('debug_raw_rgb.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "cff7e7c5d2a5e2cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw observation type: <class 'numpy.ndarray'>\n",
      "Raw observation shape: (3, 48, 48)\n",
      "Raw observation dtype: float64\n",
      "Raw observation range: [0.000, 1.000]\n",
      "Channels: 3, Height: 48, Width: 48\n",
      "Channel 0 unique values: [0.         0.12941176 0.21568627 0.21960784 0.39215686 0.44313725\n",
      " 0.55294118 0.88627451 1.        ]\n",
      "Channel 1 unique values: [0.         0.12941176 0.21568627 0.39215686 1.        ]\n",
      "Channel 2 unique values: [0.         0.12941176 0.21568627 0.39215686]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T22:38:46.879269Z",
     "start_time": "2025-07-09T22:38:46.765130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if isinstance(obs, np.ndarray):\n",
    "    obs_tensor = torch.from_numpy(obs).float()\n",
    "else:\n",
    "    obs_tensor = obs.float()\n",
    "\n",
    "print(f\"Input tensor shape: {obs_tensor.shape}\")\n",
    "print(f\"Input tensor dtype: {obs_tensor.dtype}\")\n",
    "print(f\"Input tensor range: [{obs_tensor.min():.3f}, {obs_tensor.max():.3f}]\")\n",
    "\n",
    "# Test states_to_imgs function\n",
    "\n",
    "if len(obs_tensor.shape) == 3:\n",
    "    obs_tensor = obs_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(f\"Processing tensor shape: {obs_tensor.shape}\")\n",
    "\n",
    "# Call states_to_imgs\n",
    "converted_imgs = states_to_imgs(obs_tensor, env_name='MiniGrid-Empty-6x6-v0')\n",
    "\n",
    "print(f\"Converted output type: {type(converted_imgs)}\")\n",
    "print(f\"Converted output shape: {converted_imgs.shape}\")\n",
    "print(f\"Converted output dtype: {converted_imgs.dtype}\")\n",
    "print(f\"Converted output range: [{converted_imgs.min():.3f}, {converted_imgs.max():.3f}]\")\n",
    "\n",
    "# Visualize the conversion result\n",
    "if len(converted_imgs.shape) == 4:  # Batch of images\n",
    "    img = converted_imgs[0]\n",
    "else:\n",
    "    img = converted_imgs\n",
    "\n",
    "# Handle different channel arrangements\n",
    "if len(img.shape) == 3:\n",
    "    if img.shape[0] <= 3:  # Channels first\n",
    "        img_display = img.transpose(1, 2, 0)\n",
    "    else:  # Channels last\n",
    "        img_display = img\n",
    "else:\n",
    "    img_display = img\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img_display.clip(0, 1))\n",
    "plt.title('states_to_imgs Output')\n",
    "plt.axis('off')\n",
    "plt.savefig('debug_states_to_imgs.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "b1e5af165d61c0df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([3, 48, 48])\n",
      "Input tensor dtype: torch.float32\n",
      "Input tensor range: [0.000, 1.000]\n",
      "Processing tensor shape: torch.Size([1, 3, 48, 48])\n",
      "Converted output type: <class 'numpy.ndarray'>\n",
      "Converted output shape: (1, 3, 48, 48)\n",
      "Converted output dtype: float32\n",
      "Converted output range: [0.000, 1.000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T22:50:15.378389Z",
     "start_time": "2025-07-09T22:50:15.230777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import RGBImgPartialObsWrapper, ImgObsWrapper\n",
    "from env_helpers import Custom2DWrapper\n",
    "\n",
    "# Step 1: Raw MiniGrid\n",
    "print(\"Step 1: Raw MiniGrid environment\")\n",
    "env1 = gym.make('MiniGrid-Empty-6x6-v0')\n",
    "obs1 = env1.reset()\n",
    "if isinstance(obs1, tuple):\n",
    "    obs1 = obs1[0]\n",
    "print(f\"  Raw obs type: {type(obs1)}, keys: {list(obs1.keys()) if isinstance(obs1, dict) else 'not dict'}\")\n",
    "if isinstance(obs1, dict) and 'image' in obs1:\n",
    "    print(f\"  Raw image shape: {obs1['image'].shape}\")\n",
    "\n",
    "# Step 2: RGB wrapper\n",
    "print(\"Step 2: After RGBImgPartialObsWrapper\")\n",
    "env2 = RGBImgPartialObsWrapper(env1)\n",
    "obs2 = env2.reset()\n",
    "if isinstance(obs2, tuple):\n",
    "    obs2 = obs2[0]\n",
    "print(f\"  RGB obs type: {type(obs2)}, keys: {list(obs2.keys()) if isinstance(obs2, dict) else 'not dict'}\")\n",
    "if isinstance(obs2, dict) and 'image' in obs2:\n",
    "    print(f\"  RGB image shape: {obs2['image'].shape}\")\n",
    "    print(f\"  RGB image range: [{obs2['image'].min()}, {obs2['image'].max()}]\")\n",
    "\n",
    "# Step 3: ImgObsWrapper\n",
    "print(\"Step 3: After ImgObsWrapper\")\n",
    "env3 = ImgObsWrapper(env2)\n",
    "obs3 = env3.reset()\n",
    "if isinstance(obs3, tuple):\n",
    "    obs3 = obs3[0]\n",
    "print(f\"  ImgObs type: {type(obs3)}, shape: {obs3.shape if hasattr(obs3, 'shape') else 'no shape'}\")\n",
    "if hasattr(obs3, 'shape'):\n",
    "    print(f\"  ImgObs range: [{obs3.min()}, {obs3.max()}]\")\n",
    "\n",
    "# Step 4: Custom2DWrapper\n",
    "print(\"Step 4: After Custom2DWrapper\")\n",
    "env4 = Custom2DWrapper(env3)\n",
    "obs4 = env4.reset()\n",
    "if isinstance(obs4, tuple):\n",
    "    obs4 = obs4[0]\n",
    "print(f\"  Final obs type: {type(obs4)}, shape: {obs4.shape if hasattr(obs4, 'shape') else 'no shape'}\")\n",
    "if hasattr(obs4, 'shape'):\n",
    "    print(f\"  Final obs range: [{obs4.min():.3f}, {obs4.max():.3f}]\")\n",
    "\n",
    "# Visualize the final result\n",
    "if hasattr(obs4, 'shape') and len(obs4.shape) == 3:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    if obs4.shape[0] <= 3:\n",
    "        display_img = obs4.transpose(1, 2, 0)\n",
    "    else:\n",
    "        display_img = obs4\n",
    "    plt.imshow(display_img.clip(0, 1))\n",
    "    plt.title('Final Environment Output')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('debug_final_env_output.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Clean up\n",
    "env1.close()\n",
    "env2.close()\n",
    "env3.close()\n",
    "env4.close()"
   ],
   "id": "8c19a3855b39f901",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Raw MiniGrid environment\n",
      "  Raw obs type: <class 'dict'>, keys: ['image', 'direction', 'mission']\n",
      "  Raw image shape: (7, 7, 3)\n",
      "Step 2: After RGBImgPartialObsWrapper\n",
      "  RGB obs type: <class 'dict'>, keys: ['image', 'direction', 'mission']\n",
      "  RGB image shape: (56, 56, 3)\n",
      "  RGB image range: [76, 255]\n",
      "Step 3: After ImgObsWrapper\n",
      "  ImgObs type: <class 'numpy.ndarray'>, shape: (56, 56, 3)\n",
      "  ImgObs range: [76, 255]\n",
      "Step 4: After Custom2DWrapper\n",
      "  Final obs type: <class 'numpy.ndarray'>, shape: (3, 56, 56)\n",
      "  Final obs range: [0.298, 1.000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T00:10:07.005897Z",
     "start_time": "2025-07-11T00:10:06.992174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check_model_files.py\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Update these paths to your actual file paths\n",
    "model1_path = 'models/model_25cb239f0df83f36333fa6dc2e84c913.pt'  # 974 KB\n",
    "model2_path = 'models/model_aa6d126ffa2897f2d6e76068df7ab6b4.pt'  # 5062 KB\n",
    "\n",
    "def check_model(path, name):\n",
    "    print(f\"\\n=== Checking {name} ===\")\n",
    "    print(f\"File size: {os.path.getsize(path) / 1024:.1f} KB\")\n",
    "\n",
    "    checkpoint = torch.load(path, map_location='cpu')\n",
    "\n",
    "    if isinstance(checkpoint, dict):\n",
    "        print(f\"Type: Dictionary with keys: {list(checkpoint.keys())[:5]}...\")  # First 5 keys\n",
    "    else:\n",
    "        print(f\"Type: OrderedDict with {len(checkpoint)} keys\")\n",
    "\n",
    "    # Check for telltale keys\n",
    "    all_keys = list(checkpoint.keys()) if isinstance(checkpoint, dict) else list(checkpoint.keys())\n",
    "\n",
    "    # Autoencoder typically has encoder/decoder keys\n",
    "    encoder_keys = [k for k in all_keys if 'encoder' in k or 'decoder' in k]\n",
    "\n",
    "    # Transition model typically has state/action/reward prediction keys\n",
    "    trans_keys = [k for k in all_keys if 'state' in k or 'action' in k or 'reward' in k or 'gamma' in k]\n",
    "\n",
    "    print(f\"Encoder/Decoder keys: {len(encoder_keys)}\")\n",
    "    print(f\"Transition-related keys: {len(trans_keys)}\")\n",
    "\n",
    "    # Sample some keys\n",
    "    print(f\"First few keys: {all_keys[:10]}\")\n",
    "\n",
    "    # Check for specific architecture hints\n",
    "    if 'fc_layer.weight' in all_keys:\n",
    "        print(\"Has fc_layer - likely autoencoder\")\n",
    "    if 'z_upscale_layer.weight' in all_keys:\n",
    "        print(\"Has z_upscale_layer - likely autoencoder\")\n",
    "    if any('mlp' in k for k in all_keys):\n",
    "        print(\"Has MLP layers - likely transition model\")\n",
    "\n",
    "check_model(model1_path, \"Model 1 (974 KB)\")\n",
    "check_model(model2_path, \"Model 2 (5062 KB)\")\n"
   ],
   "id": "74050de250bd57c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Checking Model 1 (974 KB) ===\n",
      "File size: 5061.0 KB\n",
      "Type: Dictionary with keys: ['encoder.0.weight', 'encoder.0.bias', 'encoder.2.weight', 'encoder.2.bias', 'encoder.4.weight']...\n",
      "Encoder/Decoder keys: 38\n",
      "Transition-related keys: 0\n",
      "First few keys: ['encoder.0.weight', 'encoder.0.bias', 'encoder.2.weight', 'encoder.2.bias', 'encoder.4.weight', 'encoder.4.bias', 'encoder.7.conv1.weight', 'encoder.7.bn1.weight', 'encoder.7.bn1.bias', 'encoder.7.bn1.running_mean']\n",
      "Has fc_layer - likely autoencoder\n",
      "Has z_upscale_layer - likely autoencoder\n",
      "\n",
      "=== Checking Model 2 (5062 KB) ===\n",
      "File size: 973.3 KB\n",
      "Type: Dictionary with keys: ['shared_layers.1.weight', 'shared_layers.1.bias', 'shared_layers.3.weight', 'shared_layers.3.bias', 'shared_layers.5.weight']...\n",
      "Encoder/Decoder keys: 0\n",
      "Transition-related keys: 12\n",
      "First few keys: ['shared_layers.1.weight', 'shared_layers.1.bias', 'shared_layers.3.weight', 'shared_layers.3.bias', 'shared_layers.5.weight', 'shared_layers.5.bias', 'state_head.0.weight', 'state_head.0.bias', 'state_head.2.weight', 'state_head.2.bias']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:30:27.619536Z",
     "start_time": "2025-07-11T22:30:25.395519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fixed MiniGrid visualization system with proper environment detection.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try to import and register MiniGrid environments\n",
    "try:\n",
    "    import minigrid\n",
    "    from minigrid import MiniGridEnv\n",
    "    print(\"✓ MiniGrid imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ MiniGrid not found. Install with: pip install minigrid\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ========== ENVIRONMENT DETECTION ==========\n",
    "\n",
    "def find_available_minigrid_envs():\n",
    "    \"\"\"Find all available MiniGrid environments\"\"\"\n",
    "    available_envs = []\n",
    "\n",
    "    # Get all registered environments\n",
    "    all_envs = gym.envs.registry.env_specs.keys()\n",
    "\n",
    "    # Filter for MiniGrid environments\n",
    "    minigrid_envs = [env for env in all_envs if 'minigrid' in env.lower() or 'mini_grid' in env.lower()]\n",
    "\n",
    "    return sorted(minigrid_envs)\n",
    "\n",
    "def suggest_correct_env_name(requested_name):\n",
    "    \"\"\"Suggest the correct environment name based on user input\"\"\"\n",
    "    available = find_available_minigrid_envs()\n",
    "\n",
    "    print(f\"\\n❌ Environment '{requested_name}' not found.\")\n",
    "    print(f\"📋 Available MiniGrid environments:\")\n",
    "\n",
    "    for i, env_name in enumerate(available[:20]):  # Show first 20\n",
    "        print(f\"  {i+1:2d}. {env_name}\")\n",
    "\n",
    "    if len(available) > 20:\n",
    "        print(f\"  ... and {len(available) - 20} more\")\n",
    "\n",
    "    # Try to find similar names\n",
    "    suggestions = []\n",
    "    req_lower = requested_name.lower()\n",
    "\n",
    "    for env in available:\n",
    "        env_lower = env.lower()\n",
    "        if 'empty' in req_lower and 'empty' in env_lower:\n",
    "            suggestions.append(env)\n",
    "        elif '6x6' in req_lower and '6x6' in env_lower:\n",
    "            suggestions.append(env)\n",
    "        elif 'minigrid' in req_lower and env_lower.startswith('minigrid'):\n",
    "            suggestions.append(env)\n",
    "\n",
    "    if suggestions:\n",
    "        print(f\"\\n💡 Did you mean one of these?\")\n",
    "        for suggestion in suggestions[:5]:\n",
    "            print(f\"  - {suggestion}\")\n",
    "\n",
    "    # Return the most likely candidate\n",
    "    if suggestions:\n",
    "        return suggestions[0]\n",
    "    elif available:\n",
    "        return available[0]  # Return first available as fallback\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def create_test_environment(env_name):\n",
    "    \"\"\"Try to create environment with error handling and suggestions\"\"\"\n",
    "    try:\n",
    "        # First try the exact name\n",
    "        env = gym.make(env_name, render_mode='rgb_array')\n",
    "        print(f\"✓ Successfully created environment: {env_name}\")\n",
    "        return env, env_name\n",
    "\n",
    "    except gym.error.NameNotFound:\n",
    "        # Try to find a similar environment\n",
    "        suggested_name = suggest_correct_env_name(env_name)\n",
    "\n",
    "        if suggested_name:\n",
    "            try:\n",
    "                env = gym.make(suggested_name, render_mode='rgb_array')\n",
    "                print(f\"✓ Using suggested environment: {suggested_name}\")\n",
    "                return env, suggested_name\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to create suggested environment {suggested_name}: {e}\")\n",
    "\n",
    "        # If all else fails, try some common ones\n",
    "        common_envs = [\n",
    "            'MiniGrid-Empty-5x5-v0',\n",
    "            'MiniGrid-Empty-8x8-v0',\n",
    "            'MiniGrid-Empty-16x16-v0',\n",
    "            'MiniGrid-DoorKey-5x5-v0',\n",
    "            'MiniGrid-FourRooms-v0'\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n🔄 Trying common MiniGrid environments...\")\n",
    "        for common_env in common_envs:\n",
    "            try:\n",
    "                env = gym.make(common_env, render_mode='rgb_array')\n",
    "                print(f\"✓ Successfully created fallback environment: {common_env}\")\n",
    "                return env, common_env\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(\"❌ Could not create any MiniGrid environment\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ========== MODEL LOADING CODE (Same as before) ==========\n",
    "\n",
    "class FixedModelLoader:\n",
    "    \"\"\"Improved model loader that handles various tensor types correctly\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model_from_checkpoint(checkpoint_path: str, device: str = 'cpu') -> torch.nn.Module:\n",
    "        \"\"\"Load a model from checkpoint, handling different save formats\"\"\"\n",
    "\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {checkpoint_path}\")\n",
    "\n",
    "        print(f\"Loading model from: {checkpoint_path}\")\n",
    "\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Case 1: Full model was saved\n",
    "        if isinstance(checkpoint, torch.nn.Module):\n",
    "            print(\"✓ Loaded full model object\")\n",
    "            return checkpoint.to(device)\n",
    "\n",
    "        # Case 2: Dictionary with state dict\n",
    "        if isinstance(checkpoint, dict):\n",
    "            state_dict = None\n",
    "\n",
    "            # Try different keys for state dict\n",
    "            for key in ['state_dict', 'model_state_dict', 'model', 'encoder_state_dict', 'model_weights']:\n",
    "                if key in checkpoint:\n",
    "                    state_dict = checkpoint[key]\n",
    "                    print(f\"Found state dict under key: '{key}'\")\n",
    "                    break\n",
    "\n",
    "            if state_dict is None:\n",
    "                # Check if the entire checkpoint looks like a state dict\n",
    "                if FixedModelLoader._looks_like_state_dict(checkpoint):\n",
    "                    state_dict = checkpoint\n",
    "                    print(\"Using entire checkpoint as state dict\")\n",
    "                else:\n",
    "                    print(\"Available keys in checkpoint:\")\n",
    "                    for key, value in checkpoint.items():\n",
    "                        print(f\"  {key}: {type(value)}\")\n",
    "                    raise ValueError(\"Could not find state dict in checkpoint\")\n",
    "\n",
    "            print(f\"State dict has {len(state_dict)} parameters\")\n",
    "            print(\"Sample parameter names:\", list(state_dict.keys())[:5])\n",
    "\n",
    "            return FixedModelLoader._create_safe_model(state_dict, device)\n",
    "\n",
    "        raise ValueError(f\"Cannot determine how to load model from {checkpoint_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _looks_like_state_dict(checkpoint):\n",
    "        \"\"\"Check if a dictionary looks like a state dict\"\"\"\n",
    "        if not isinstance(checkpoint, dict):\n",
    "            return False\n",
    "\n",
    "        # Check if all values are tensors\n",
    "        tensor_count = sum(1 for v in checkpoint.values() if isinstance(v, torch.Tensor))\n",
    "        total_count = len(checkpoint)\n",
    "\n",
    "        # If more than 80% are tensors, probably a state dict\n",
    "        return tensor_count / total_count > 0.8 if total_count > 0 else False\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_safe_model(state_dict: Dict[str, torch.Tensor], device: str) -> torch.nn.Module:\n",
    "        \"\"\"Create a model that safely handles all tensor types\"\"\"\n",
    "\n",
    "        class SafeModel(nn.Module):\n",
    "            def __init__(self, state_dict):\n",
    "                super().__init__()\n",
    "\n",
    "                # Infer properties from state dict\n",
    "                self._infer_model_properties(state_dict)\n",
    "\n",
    "                # Process parameters carefully\n",
    "                self._create_parameters_safely(state_dict)\n",
    "\n",
    "                # Set up model type detection\n",
    "                self._detect_model_type(state_dict)\n",
    "\n",
    "            def _infer_model_properties(self, state_dict):\n",
    "                \"\"\"Infer model properties from state dict\"\"\"\n",
    "                self.latent_dim = 64\n",
    "                self.n_embeddings = 512\n",
    "                self.n_latent_embeds = 64\n",
    "                self.embedding_dim = 64\n",
    "\n",
    "                # Try to infer dimensions from state dict\n",
    "                for name, tensor in state_dict.items():\n",
    "                    if 'embed' in name.lower() and len(tensor.shape) == 2:\n",
    "                        self.n_embeddings, self.embedding_dim = tensor.shape\n",
    "                        self.latent_dim = self.embedding_dim\n",
    "                    elif 'quantize' in name.lower() and len(tensor.shape) >= 1:\n",
    "                        if len(tensor.shape) == 2:\n",
    "                            self.n_latent_embeds = tensor.shape[0]\n",
    "                        else:\n",
    "                            self.n_latent_embeds = tensor.shape[-1]\n",
    "\n",
    "                print(f\"Inferred properties: latent_dim={self.latent_dim}, n_embeddings={self.n_embeddings}\")\n",
    "\n",
    "            def _create_parameters_safely(self, state_dict):\n",
    "                \"\"\"Safely create parameters handling different tensor types\"\"\"\n",
    "                self.param_dict = nn.ParameterDict()\n",
    "                self.buffer_dict = {}\n",
    "\n",
    "                for name, tensor in state_dict.items():\n",
    "                    safe_name = name.replace('.', '_').replace('/', '_')\n",
    "\n",
    "                    # Check tensor properties\n",
    "                    is_floating_point = tensor.dtype.is_floating_point\n",
    "                    is_complex = tensor.dtype.is_complex\n",
    "                    requires_grad = hasattr(tensor, 'requires_grad') and tensor.requires_grad\n",
    "\n",
    "                    try:\n",
    "                        if is_floating_point or is_complex:\n",
    "                            # Can be a parameter\n",
    "                            if requires_grad:\n",
    "                                self.param_dict[safe_name] = nn.Parameter(tensor.clone())\n",
    "                            else:\n",
    "                                # Register as buffer for floating point tensors that don't need gradients\n",
    "                                self.register_buffer(f'buffer_{safe_name}', tensor.clone())\n",
    "                                self.buffer_dict[safe_name] = f'buffer_{safe_name}'\n",
    "                        else:\n",
    "                            # Integer or other types - must be buffers\n",
    "                            self.register_buffer(f'buffer_{safe_name}', tensor.clone())\n",
    "                            self.buffer_dict[safe_name] = f'buffer_{safe_name}'\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not process parameter {name}: {e}\")\n",
    "                        # Store in a simple dict as fallback\n",
    "                        self.buffer_dict[safe_name] = tensor.clone()\n",
    "\n",
    "            def _detect_model_type(self, state_dict):\n",
    "                \"\"\"Detect what type of model this is\"\"\"\n",
    "                keys = list(state_dict.keys())\n",
    "                key_str = ' '.join(keys).lower()\n",
    "\n",
    "                self.is_vqvae = any(indicator in key_str for indicator in ['quantize', 'embedding', 'codebook', 'vq'])\n",
    "                self.is_transformer = any(indicator in key_str for indicator in ['transformer', 'attention', 'self_attn'])\n",
    "                self.is_transition = any(indicator in key_str for indicator in ['transition', 'dynamics', 'reward', 'gamma'])\n",
    "\n",
    "                print(f\"Model type detection: VQ-VAE={self.is_vqvae}, Transformer={self.is_transformer}, Transition={self.is_transition}\")\n",
    "\n",
    "            def encode(self, x):\n",
    "                \"\"\"Encoding method - adapt based on model type\"\"\"\n",
    "                if self.is_vqvae:\n",
    "                    return self._vqvae_encode(x)\n",
    "                else:\n",
    "                    # Generic encoding - just flatten\n",
    "                    return x.view(x.size(0), -1)\n",
    "\n",
    "            def decode(self, z):\n",
    "                \"\"\"Decoding method - adapt based on model type\"\"\"\n",
    "                if self.is_vqvae:\n",
    "                    return self._vqvae_decode(z)\n",
    "                else:\n",
    "                    # Generic decoding - return as is\n",
    "                    return z\n",
    "\n",
    "            def _vqvae_encode(self, x):\n",
    "                \"\"\"VQ-VAE style encoding\"\"\"\n",
    "                batch_size = x.size(0)\n",
    "\n",
    "                # Try to find embedding dimensions\n",
    "                if hasattr(self, 'n_latent_embeds') and hasattr(self, 'n_embeddings'):\n",
    "                    # Return random indices for now (in real implementation, this would be learned)\n",
    "                    return torch.randint(0, self.n_embeddings,\n",
    "                                       (batch_size, self.n_latent_embeds),\n",
    "                                       device=x.device)\n",
    "                else:\n",
    "                    # Fallback to flattened representation\n",
    "                    return x.view(batch_size, -1)\n",
    "\n",
    "            def _vqvae_decode(self, z):\n",
    "                \"\"\"VQ-VAE style decoding\"\"\"\n",
    "                if z.dtype == torch.long:\n",
    "                    # If z contains indices, convert to float\n",
    "                    return z.float()\n",
    "                else:\n",
    "                    return z\n",
    "\n",
    "            def forward(self, x, action=None, return_logits=False):\n",
    "                \"\"\"Forward pass - adapt based on usage\"\"\"\n",
    "                if action is not None:\n",
    "                    # This is being used as a transition model\n",
    "                    return self._transition_forward(x, action, return_logits)\n",
    "                else:\n",
    "                    # This is being used as an autoencoder\n",
    "                    encoded = self.encode(x)\n",
    "                    return self.decode(encoded)\n",
    "\n",
    "            def _transition_forward(self, state, action, return_logits=False):\n",
    "                \"\"\"Transition model forward pass\"\"\"\n",
    "                batch_size = state.size(0)\n",
    "                device = state.device\n",
    "\n",
    "                # Simple transition: slight perturbation of current state\n",
    "                if state.dtype == torch.long:\n",
    "                    # For discrete states, randomly change a few indices\n",
    "                    next_state = state.clone()\n",
    "                    # Randomly flip some indices\n",
    "                    mask = torch.rand_like(state.float()) < 0.1  # 10% chance to change\n",
    "                    random_indices = torch.randint_like(state, 0, self.n_embeddings)\n",
    "                    next_state = torch.where(mask, random_indices, next_state)\n",
    "                else:\n",
    "                    # For continuous states, add small noise\n",
    "                    noise = torch.randn_like(state) * 0.1\n",
    "                    next_state = state + noise\n",
    "\n",
    "                # Dummy reward and gamma predictions\n",
    "                reward = torch.zeros(batch_size, 1, device=device)\n",
    "                gamma = torch.ones(batch_size, 1, device=device) * 0.99\n",
    "\n",
    "                if return_logits:\n",
    "                    return next_state, reward, gamma\n",
    "                return next_state, reward, gamma\n",
    "\n",
    "            def logits_to_state(self, logits):\n",
    "                \"\"\"Convert logits to state representation\"\"\"\n",
    "                if self.is_vqvae and logits.dtype.is_floating_point:\n",
    "                    # For VQ-VAE, convert logits to indices\n",
    "                    return torch.argmax(logits, dim=-1)\n",
    "                return logits\n",
    "\n",
    "        model = SafeModel(state_dict)\n",
    "\n",
    "        # Load the state dict properly\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"✓ Loaded state dict with some flexibility\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load state dict perfectly: {e}\")\n",
    "            print(\"Model will work with inferred parameters\")\n",
    "\n",
    "        return model.to(device)\n",
    "\n",
    "\n",
    "# ========== VISUALIZATION CODE ==========\n",
    "\n",
    "class MiniGridVisualizer:\n",
    "    \"\"\"Visualizes MiniGrid environments with real vs predicted comparisons\"\"\"\n",
    "\n",
    "    def __init__(self, env_name: str):\n",
    "        self.env_name = env_name\n",
    "\n",
    "        # Try to create environment with proper error handling\n",
    "        self.env, self.actual_env_name = create_test_environment(env_name)\n",
    "        if self.env is None:\n",
    "            raise ValueError(f\"Could not create any MiniGrid environment\")\n",
    "\n",
    "        # Action names for MiniGrid\n",
    "        self.action_names = {\n",
    "            0: \"Turn Left\",\n",
    "            1: \"Turn Right\",\n",
    "            2: \"Move Forward\",\n",
    "            3: \"Pick Up\",\n",
    "            4: \"Drop\",\n",
    "            5: \"Toggle\",\n",
    "            6: \"Done\"\n",
    "        }\n",
    "\n",
    "        print(f\"✓ Environment created: {self.actual_env_name}\")\n",
    "        print(f\"Action space: {self.env.action_space}\")\n",
    "\n",
    "        # Test the environment\n",
    "        try:\n",
    "            obs, info = self.env.reset()\n",
    "            print(f\"✓ Environment works, observation shape: {obs.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Environment test failed: {e}\")\n",
    "\n",
    "    def obs_to_grid_image(self, obs, title=\"Grid\"):\n",
    "        \"\"\"Convert MiniGrid observation to a readable grid image\"\"\"\n",
    "        if isinstance(obs, torch.Tensor):\n",
    "            obs = obs.cpu().numpy()\n",
    "\n",
    "        # Handle different observation formats\n",
    "        if len(obs.shape) == 4:  # Batch of observations\n",
    "            obs = obs[0]  # Take first in batch\n",
    "\n",
    "        if len(obs.shape) == 3:\n",
    "            if obs.shape[0] == 3:  # RGB format (C, H, W)\n",
    "                obs = obs.transpose(1, 2, 0)  # Convert to (H, W, C)\n",
    "            # If shape is (H, W, C), it's already correct\n",
    "\n",
    "        # Ensure values are in [0, 1] range for display\n",
    "        if obs.max() > 1.0:\n",
    "            obs = obs / 255.0\n",
    "\n",
    "        return obs.clip(0, 1)\n",
    "\n",
    "    def create_comparison_plot(self, real_obs_list, pred_obs_list, actions_taken,\n",
    "                             rewards_real, rewards_pred, title=\"Real vs Predicted\"):\n",
    "        \"\"\"Create a side-by-side comparison plot\"\"\"\n",
    "\n",
    "        n_steps = len(real_obs_list)\n",
    "        fig = plt.figure(figsize=(4 * n_steps, 8))\n",
    "\n",
    "        # Create grid layout: 3 rows (real, predicted, action) x n_steps columns\n",
    "        gs = GridSpec(3, n_steps, figure=fig, height_ratios=[1, 1, 0.3])\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            # Real observation (top row)\n",
    "            ax_real = fig.add_subplot(gs[0, step])\n",
    "            real_img = self.obs_to_grid_image(real_obs_list[step])\n",
    "            ax_real.imshow(real_img)\n",
    "            ax_real.set_title(f\"Real Step {step}\\nR: {rewards_real[step]:.2f}\")\n",
    "            ax_real.axis('off')\n",
    "\n",
    "            # Predicted observation (middle row)\n",
    "            ax_pred = fig.add_subplot(gs[1, step])\n",
    "            pred_img = self.obs_to_grid_image(pred_obs_list[step])\n",
    "            ax_pred.imshow(pred_img)\n",
    "            ax_pred.set_title(f\"Predicted Step {step}\\nR: {rewards_pred[step]:.2f}\")\n",
    "            ax_pred.axis('off')\n",
    "\n",
    "            # Action taken (bottom row)\n",
    "            if step < len(actions_taken):\n",
    "                ax_action = fig.add_subplot(gs[2, step])\n",
    "                action_name = self.action_names.get(actions_taken[step], f\"Action {actions_taken[step]}\")\n",
    "                ax_action.text(0.5, 0.5, action_name, ha='center', va='center',\n",
    "                             fontsize=10, weight='bold')\n",
    "                ax_action.set_xlim(0, 1)\n",
    "                ax_action.set_ylim(0, 1)\n",
    "                ax_action.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"{title}\\nEnvironment: {self.actual_env_name}\", fontsize=16, y=0.95)\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def run_comparative_rollout(self, encoder_model, trans_model, device: str,\n",
    "                               n_steps: int = 5, n_episodes: int = 3):\n",
    "        \"\"\"Run rollouts comparing real environment with model predictions\"\"\"\n",
    "\n",
    "        print(f\"\\n=== MiniGrid Comparative Rollout ({n_episodes} episodes) ===\")\n",
    "        print(f\"Environment: {self.actual_env_name}\")\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            print(f\"\\nEpisode {episode + 1}/{n_episodes}\")\n",
    "\n",
    "            # Reset environment\n",
    "            try:\n",
    "                reset_result = self.env.reset()\n",
    "                if isinstance(reset_result, tuple):\n",
    "                    obs, info = reset_result\n",
    "                else:\n",
    "                    obs = reset_result\n",
    "                    info = {}\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to reset environment: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Store observations and actions\n",
    "            real_obs_list = [obs.copy()]\n",
    "            pred_obs_list = []\n",
    "            actions_taken = []\n",
    "            rewards_real = [0.0]  # Initial reward is 0\n",
    "            rewards_pred = []\n",
    "\n",
    "            # Convert initial observation to tensor\n",
    "            current_obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "            current_state = encoder_model.encode(current_obs_tensor)\n",
    "\n",
    "            print(f\"Initial state shape: {current_state.shape}, dtype: {current_state.dtype}\")\n",
    "\n",
    "            episode_ended = False\n",
    "\n",
    "            for step in range(n_steps):\n",
    "                if episode_ended:\n",
    "                    break\n",
    "\n",
    "                # Sample action\n",
    "                action = self.env.action_space.sample()\n",
    "                actions_taken.append(action)\n",
    "\n",
    "                # == MODEL PREDICTION ==\n",
    "                try:\n",
    "                    action_tensor = torch.LongTensor([action]).to(device)\n",
    "\n",
    "                    # Predict next state, reward, gamma\n",
    "                    next_state_pred, reward_pred, gamma_pred = trans_model(current_state, action_tensor)\n",
    "\n",
    "                    # Decode predicted state to observation\n",
    "                    pred_obs_tensor = encoder_model.decode(next_state_pred)\n",
    "\n",
    "                    # Convert to numpy for visualization\n",
    "                    if isinstance(pred_obs_tensor, torch.Tensor):\n",
    "                        pred_obs = pred_obs_tensor.cpu().detach().numpy()[0]\n",
    "                    else:\n",
    "                        pred_obs = pred_obs_tensor[0] if hasattr(pred_obs_tensor, '__getitem__') else pred_obs_tensor\n",
    "\n",
    "                    # Ensure proper shape for MiniGrid\n",
    "                    if len(pred_obs.shape) == 1:\n",
    "                        # If flattened, try to reshape to original observation shape\n",
    "                        try:\n",
    "                            pred_obs = pred_obs.reshape(obs.shape)\n",
    "                        except:\n",
    "                            # If reshape fails, create a dummy observation with same shape\n",
    "                            pred_obs = np.zeros_like(obs)\n",
    "                    elif len(pred_obs.shape) == 3 and pred_obs.shape[0] == 3:\n",
    "                        # If in CHW format, convert to HWC\n",
    "                        pred_obs = pred_obs.transpose(1, 2, 0)\n",
    "\n",
    "                    # Ensure pred_obs has the same shape as obs\n",
    "                    if pred_obs.shape != obs.shape:\n",
    "                        print(f\"  Warning: reshaping prediction from {pred_obs.shape} to {obs.shape}\")\n",
    "                        pred_obs = np.zeros_like(obs)  # Fallback\n",
    "\n",
    "                    pred_obs_list.append(pred_obs)\n",
    "                    rewards_pred.append(reward_pred.item() if hasattr(reward_pred, 'item') else float(reward_pred))\n",
    "\n",
    "                    print(f\"  Predicted reward: {rewards_pred[-1]:.3f}, gamma: {gamma_pred.item():.3f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Model prediction failed: {e}\")\n",
    "                    # Use previous observation as fallback\n",
    "                    pred_obs_list.append(real_obs_list[-1].copy())\n",
    "                    rewards_pred.append(0.0)\n",
    "\n",
    "                # == REAL ENVIRONMENT STEP ==\n",
    "                try:\n",
    "                    step_result = self.env.step(action)\n",
    "\n",
    "                    if len(step_result) == 5:  # New gym API\n",
    "                        obs, reward, terminated, truncated, info = step_result\n",
    "                        done = terminated or truncated\n",
    "                    elif len(step_result) == 4:  # Old gym API\n",
    "                        obs, reward, done, info = step_result\n",
    "                    else:\n",
    "                        print(f\"Unexpected step result format: {step_result}\")\n",
    "                        break\n",
    "\n",
    "                    real_obs_list.append(obs.copy())\n",
    "                    rewards_real.append(reward)\n",
    "\n",
    "                    print(f\"  Real reward: {reward:.3f}, done: {done}\")\n",
    "                    print(f\"  Action: {self.action_names.get(action, f'Action {action}')}\")\n",
    "\n",
    "                    # Update current state for next prediction\n",
    "                    if not done:\n",
    "                        current_obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "                        current_state = encoder_model.encode(current_obs_tensor)\n",
    "                    else:\n",
    "                        episode_ended = True\n",
    "                        print(f\"  Episode ended at step {step + 1}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Environment step failed: {e}\")\n",
    "                    episode_ended = True\n",
    "                    break\n",
    "\n",
    "            # Store results for this episode\n",
    "            episode_result = {\n",
    "                'real_obs': real_obs_list,\n",
    "                'pred_obs': pred_obs_list,\n",
    "                'actions': actions_taken,\n",
    "                'rewards_real': rewards_real,\n",
    "                'rewards_pred': rewards_pred,\n",
    "                'episode': episode\n",
    "            }\n",
    "            all_results.append(episode_result)\n",
    "\n",
    "            # Create visualization for this episode\n",
    "            if len(pred_obs_list) > 0:\n",
    "                # Comparison plot\n",
    "                comparison_fig = self.create_comparison_plot(\n",
    "                    real_obs_list[1:len(pred_obs_list)+1],  # Skip initial obs, match pred length\n",
    "                    pred_obs_list,\n",
    "                    actions_taken,\n",
    "                    rewards_real[1:len(pred_obs_list)+1],\n",
    "                    rewards_pred,\n",
    "                    title=f\"Episode {episode + 1}: Real vs Predicted\"\n",
    "                )\n",
    "\n",
    "                # Save plot\n",
    "                save_path = f\"minigrid_comparison_episode_{episode + 1}.png\"\n",
    "                comparison_fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "                print(f\"  Saved comparison plot: {save_path}\")\n",
    "\n",
    "                plt.close('all')  # Close figures to save memory\n",
    "\n",
    "        self.env.close()\n",
    "        return all_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"MiniGrid Model Visualization\")\n",
    "    parser.add_argument('--encoder_path', required=True, help='Path to encoder model')\n",
    "    parser.add_argument('--trans_path', required=True, help='Path to transition model')\n",
    "    parser.add_argument('--env_name', default='MiniGrid-Empty-6x6-v0', help='MiniGrid environment name')\n",
    "    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    parser.add_argument('--n_steps', type=int, default=8, help='Number of steps per episode')\n",
    "    parser.add_argument('--n_episodes', type=int, default=3, help='Number of episodes to visualize')\n",
    "    parser.add_argument('--list_envs', action='store_true', help='List available MiniGrid environments')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # List environments if requested\n",
    "    if args.list_envs:\n",
    "        print(\"Available MiniGrid environments:\")\n",
    "        envs = find_available_minigrid_envs()\n",
    "        for i, env in enumerate(envs):\n",
    "            print(f\"  {i+1:2d}. {env}\")\n",
    "        return\n",
    "\n",
    "    print(\"MiniGrid Model Visualization\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Load models\n",
    "    loader = FixedModelLoader()\n",
    "\n",
    "    try:\n",
    "        encoder_model = loader.load_model_from_checkpoint(args.encoder_path, args.device)\n",
    "        trans_model = loader.load_model_from_checkpoint(args.trans_path, args.device)\n",
    "\n",
    "        encoder_model.eval()\n",
    "        trans_model.eval()\n",
    "\n",
    "        print(\"✓ Models loaded successfully\")\n",
    "\n",
    "        # Create visualizer\n",
    "        visualizer = MiniGridVisualizer(args.env_name)\n",
    "\n",
    "        # Run comparative rollouts\n",
    "        results = visualizer.run_comparative_rollout(\n",
    "            encoder_model, trans_model, args.device, args.n_steps, args.n_episodes\n",
    "        )\n",
    "\n",
    "        print(f\"\\n✓ Generated visualizations for {len(results)} episodes\")\n",
    "        print(\"Check the generated PNG files for detailed comparisons!\")\n",
    "\n",
    "        # Print summary statistics\n",
    "        if results:\n",
    "            print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "\n",
    "            total_steps = sum(len(r['actions']) for r in results)\n",
    "            total_real_reward = sum(sum(r['rewards_real']) for r in results)\n",
    "            total_pred_reward = sum(sum(r['rewards_pred']) for r in results)\n",
    "\n",
    "            print(f\"Environment used: {visualizer.actual_env_name}\")\n",
    "            print(f\"Total steps across all episodes: {total_steps}\")\n",
    "            print(f\"Total real reward: {total_real_reward:.3f}\")\n",
    "            print(f\"Total predicted reward: {total_pred_reward:.3f}\")\n",
    "            print(f\"Reward prediction error: {abs(total_real_reward - total_pred_reward):.3f}\")\n",
    "\n",
    "            # Calculate average MSE across all predictions\n",
    "            all_mses = []\n",
    "            for result in results:\n",
    "                if len(result['pred_obs']) > 0:\n",
    "                    real_obs = np.array(result['real_obs'][1:len(result['pred_obs'])+1])\n",
    "                    pred_obs = np.array(result['pred_obs'])\n",
    "\n",
    "                    if real_obs.shape == pred_obs.shape:\n",
    "                        mse = np.mean((real_obs - pred_obs) ** 2)\n",
    "                        all_mses.append(mse)\n",
    "\n",
    "            if all_mses:\n",
    "                avg_mse = np.mean(all_mses)\n",
    "                print(f\"Average observation prediction MSE: {avg_mse:.6f}\")\n",
    "\n",
    "                if avg_mse < 0.01:\n",
    "                    print(\"🎉 Excellent prediction quality!\")\n",
    "                elif avg_mse < 0.1:\n",
    "                    print(\"✅ Good prediction quality\")\n",
    "                elif avg_mse < 0.5:\n",
    "                    print(\"⚠️  Moderate prediction quality\")\n",
    "                else:\n",
    "                    print(\"❌ Poor prediction quality\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        print(f\"\\n💡 Try:\")\n",
    "        print(f\"1. List available environments: python {sys.argv[0]} --list_envs\")\n",
    "        print(f\"2. Install MiniGrid: pip install minigrid\")\n",
    "        print(f\"3. Use a different environment name\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "2af55dce6640ff72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ MiniGrid not found. Install with: pip install minigrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiar3\\AppData\\Roaming\\Python\\Python313\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[31mSystemExit\u001B[39m\u001B[31m:\u001B[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiar3\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "acbc88fe6fed3a14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
